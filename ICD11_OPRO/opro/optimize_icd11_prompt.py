"""
ICD-11 OPRO (Optimization by PROmpting) Core Module

This module implements the core OPRO optimization logic for mental health prompts.
"""

import json
import os
import random
import re
import time
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional
from dataclasses import dataclass, asdict
import anthropic
from tqdm import tqdm

# ====== ‰ΩøÁî®Llama 3Ê®°ÂûãÈÄ≤Ë°åOPROÂÑ™Âåñ ======
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
    import torch
    _llama3_pipeline = None
    
    def call_local_llm(prompt, max_new_tokens=512, temperature=0.7):
        global _llama3_pipeline
        if _llama3_pipeline is None:
            print("Loading local Llama 3 model for OPRO optimization...")
            device = "cuda" if torch.cuda.is_available() else "cpu"
            
            # ‰ΩøÁî®Llama 3Ê®°Âûã
            model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
            
            tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                padding_side="left"
            )
            
            # Ë®≠ÁΩÆpad_tokenÁÇ∫eos_tokenÔºàLlama 3ÈúÄË¶ÅÔºâ
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            model = AutoModelForCausalLM.from_pretrained(
                model_name,
                device_map=device,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                low_cpu_mem_usage=True
            )
            
            _llama3_pipeline = pipeline(
                "text-generation",
                model=model,
                tokenizer=tokenizer,
                max_new_tokens=max_new_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                pad_token_id=tokenizer.pad_token_id,
                eos_token_id=tokenizer.eos_token_id,
                return_full_text=False
            )
            print("Llama 3 model loaded successfully for OPRO!")
        
        # ‰ΩøÁî®Llama 3ÁöÑinstructÊ†ºÂºè
        formatted_prompt = f"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n\n{prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n"
        result = _llama3_pipeline(formatted_prompt)
        return result[0]['generated_text'] if result else ""
        
except ImportError:
    def call_local_llm(prompt, max_new_tokens=512, temperature=0.7):
        raise RuntimeError("transformers Êàñ torch Êú™ÂÆâË£ùÔºåÁÑ°Ê≥ï‰ΩøÁî®Êú¨Âú∞Llama 3Ê®°Âûã")

# Configure logging
logging.basicConfig(level=logging.INFO) 
logger = logging.getLogger(__name__)

@dataclass
class PromptCandidate:
    """Represents a prompt candidate with metadata"""
    content: str
    score: float
    iteration: int
    parent_id: Optional[str] = None
    generation_method: str = "seed"
    timestamp: str = None
    evaluation_details: Dict[str, float] = None
    
    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()
        if self.evaluation_details is None:
            self.evaluation_details = {}

@dataclass
class OptimizationResult:
    """Stores the result of an optimization run"""
    best_prompt: PromptCandidate
    optimization_history: List[PromptCandidate]
    total_iterations: int
    improvement_achieved: float
    final_score: float
    time_elapsed: float

class OPROOptimizer:
    """Main OPRO optimizer for ICD-11 mental health prompts"""
    
    def __init__(self, config_path: str = "config.json"):
        """Initialize the OPRO optimizer with configuration"""
        self.config = self._load_config(config_path)
        self.optimization_history = []
        self.current_best = None
        self.iteration_count = 0
        
        # Initialize API clients
        self._setup_api_clients()
        
        # Load seed prompts
        self.seed_prompts = self._load_seed_prompts()
        
        # Create output directories
        os.makedirs("prompts", exist_ok=True)
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load configuration from JSON file"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except FileNotFoundError:
            logger.error(f"Configuration file {config_path} not found!")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"Invalid JSON in configuration file: {e}")
            raise
    
    
    def _load_seed_prompts(self) -> List[str]:
        """Load all seed prompts from the seed_prompts directory"""
        seed_prompts = []
        seed_dir = "opro/seed_prompts"
        
        if not os.path.exists(seed_dir):
            logger.error(f"Seed prompts directory {seed_dir} not found!")
            return []
        
        for filename in os.listdir(seed_dir):
            if filename.endswith('.txt'):
                filepath = os.path.join(seed_dir, filename)
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            seed_prompts.append(content)
                            logger.info(f"Loaded seed prompt: {filename}")
                except Exception as e:
                    logger.error(f"Failed to load seed prompt {filename}: {e}")
        
        return seed_prompts
    
    def optimize_prompts(self) -> OptimizationResult:
        """Main optimization loop using OPRO methodology"""
        logger.info("üöÄ Starting OPRO optimization process...")
        start_time = time.time()
        
        # Initialize with seed prompts
        self._initialize_with_seeds()
        
        # Run optimization iterations
        for iteration in range(self.config["opro_settings"]["max_iterations"]):
            self.iteration_count = iteration + 1
            logger.info(f"Running iteration {self.iteration_count}/{self.config['opro_settings']['max_iterations']}")
            
            # Generate new prompt candidates
            new_candidates = self._generate_prompt_variants()
            
            # Evaluate candidates
            for candidate in new_candidates:
                score = self._evaluate_prompt_candidate(candidate)
                candidate.score = score
                candidate.iteration = self.iteration_count
                
                # Update best prompt if improved
                if self.current_best is None or score > self.current_best.score:
                    self.current_best = candidate
                    logger.info(f"‚ú® New best score: {score:.3f} (improvement: {score - (self.optimization_history[-1].score if self.optimization_history else 0):.3f})")
                
                self.optimization_history.append(candidate)
            
            # Check for early stopping
            if self._should_stop_early():
                logger.info("üõë Early stopping criteria met")
                break
            
            # Rate limiting
            time.sleep(self.config["api_settings"]["rate_limit_delay"])
        
        end_time = time.time()
        time_elapsed = end_time - start_time
        
        # Save results
        result = OptimizationResult(
            best_prompt=self.current_best,
            optimization_history=self.optimization_history,
            total_iterations=self.iteration_count,
            improvement_achieved=self._calculate_improvement(),
            final_score=self.current_best.score if self.current_best else 0,
            time_elapsed=time_elapsed
        )
        
        self._save_optimization_results(result)
        
        logger.info(f"Optimization completed! Best score: {result.final_score:.3f}")
        return result
    
    def _initialize_with_seeds(self):
        """Initialize optimization with seed prompts"""
        logger.info("üå± Evaluating seed prompts...")
        
        for i, seed_prompt in enumerate(self.seed_prompts):
            candidate = PromptCandidate(
                content=seed_prompt,
                score=0.0,
                iteration=0,
                generation_method=f"seed_{i+1}"
            )
            
            score = self._evaluate_prompt_candidate(candidate)
            candidate.score = score
            
            if self.current_best is None or score > self.current_best.score:
                self.current_best = candidate
            
            self.optimization_history.append(candidate)
            logger.info(f"Seed {i+1} score: {score:.3f}")
    
    def _generate_prompt_variants(self) -> List[PromptCandidate]:
        """Generate new prompt variants using meta-LLM or offline mode"""
        logger.info("üîÑ Generating prompt variants...")
        
        # Ê™¢Êü•ÊòØÂê¶ÁÇ∫Èõ¢Á∑öÊ®°Âºè
        if (self.config["opro_settings"].get("use_offline_mode", False) or 
            self.config["opro_settings"]["meta_llm_model"] == "offline"):
            return self._generate_offline_prompt_variants()
        
        # Create meta-prompt for optimization
        meta_prompt = self._create_meta_optimization_prompt()
        
        try:
            # Generate variants using meta-LLM
            response = self._call_meta_llm(meta_prompt)
            variants = self._parse_meta_llm_response(response)
            
            # Create candidate objects
            candidates = []
            for i, variant in enumerate(variants):
                candidate = PromptCandidate(
                    content=variant,
                    score=0.0,
                    iteration=self.iteration_count,
                    parent_id=self.current_best.content[:50] if self.current_best else None,
                    generation_method="meta_llm_generation"
                )
                candidates.append(candidate)
            
            logger.info(f"Generated {len(candidates)} prompt variants")
            return candidates
            
        except Exception as e:
            logger.error(f"Failed to generate variants: {e}")
            logger.info("üîÑ Falling back to offline mode")
            return self._generate_offline_prompt_variants()
    
    def _create_meta_optimization_prompt(self) -> str:
        """Create the meta-prompt for optimization"""
        
        # Get recent optimization history
        recent_history = self.optimization_history[-5:] if len(self.optimization_history) >= 5 else self.optimization_history
        
        # Format performance analysis
        performance_analysis = self._analyze_recent_performance(recent_history)
        
        meta_prompt = f"""
You are an expert prompt engineer specializing in mental health AI systems. Your task is to optimize prompts for an ICD-11 based mental health support chatbot.

CURRENT BEST PROMPT (Score: {self.current_best.score:.3f}):
{self.current_best.content if self.current_best else "No current best"}

PERFORMANCE ANALYSIS:
{performance_analysis}

OPTIMIZATION OBJECTIVES:
1. Improve empathy and emotional connection (weight: {self.config['evaluation']['weights']['empathy']})
2. Enhance clinical accuracy and ICD-11 compliance (weight: {self.config['evaluation']['weights']['accuracy']})
3. Increase response relevance (weight: {self.config['evaluation']['weights']['relevance']})
4. Strengthen safety protocols (weight: {self.config['evaluation']['weights']['safety']})

REQUIREMENTS:
- Length: {self.config['prompt_structure']['min_prompt_length']}-{self.config['prompt_structure']['max_prompt_length']} characters
- Must include: {', '.join(self.config['prompt_structure']['required_sections'])}
- Must handle crisis situations appropriately
- Must include safety disclaimers

Please generate 3 improved prompt variants that address the performance gaps identified above. Each variant should:
1. Build upon the strengths of the current best prompt
2. Address specific weaknesses identified in the analysis
3. Experiment with different approaches (e.g., structure, tone, instructions)

Format your response as:

VARIANT 1:
[Full prompt text here]

VARIANT 2:
[Full prompt text here]

VARIANT 3:
[Full prompt text here]

OPTIMIZATION RATIONALE:
[Brief explanation of the changes made and expected improvements]
"""
        return meta_prompt
    
    def _analyze_recent_performance(self, recent_history: List[PromptCandidate]) -> str:
        """Analyze recent performance to guide optimization"""
        if not recent_history:
            return "No performance history available yet."
        
        scores = [candidate.score for candidate in recent_history]
        avg_score = sum(scores) / len(scores)
        
        # Analyze evaluation details if available
        evaluation_breakdown = {}
        for candidate in recent_history:
            if candidate.evaluation_details:
                for metric, score in candidate.evaluation_details.items():
                    if metric not in evaluation_breakdown:
                        evaluation_breakdown[metric] = []
                    evaluation_breakdown[metric].append(score)
        
        analysis = f"Average recent score: {avg_score:.3f}\n"
        
        if evaluation_breakdown:
            analysis += "Performance by metric:\n"
            for metric, scores in evaluation_breakdown.items():
                avg_metric_score = sum(scores) / len(scores)
                analysis += f"- {metric}: {avg_metric_score:.3f}\n"
        
        # Identify trends
        if len(scores) >= 2:
            trend = "improving" if scores[-1] > scores[0] else "declining" if scores[-1] < scores[0] else "stable"
            analysis += f"Recent trend: {trend}\n"
        
        return analysis
    
    
    def _parse_meta_llm_response(self, response: str) -> List[str]:
        """Parse meta-LLM response to extract prompt variants"""
        variants = []
        
        # Look for VARIANT patterns
        variant_pattern = r'VARIANT\s+\d+:\s*(.*?)(?=VARIANT\s+\d+:|OPTIMIZATION RATIONALE:|$)'
        matches = re.findall(variant_pattern, response, re.DOTALL | re.IGNORECASE)
        
        for match in matches:
            variant = match.strip()
            if len(variant) >= self.config['prompt_structure']['min_prompt_length']:
                variants.append(variant)
        
        # Fallback: split by common delimiters if pattern matching fails
        if not variants:
            potential_variants = response.split('\n\n')
            for variant in potential_variants:
                if len(variant.strip()) >= self.config['prompt_structure']['min_prompt_length']:
                    variants.append(variant.strip())
        
        return variants[:3]  # Limit to 3 variants
    
    def _generate_offline_variants(self) -> str:
        """ÁîüÊàêÈõ¢Á∑öÊ®°ÂºèÁöÑËÆäÈ´îÂª∫Ë≠∞"""
        if not self.current_best:
            return "VARIANT 1:\n‰ΩøÁî®Êõ¥ÂÖ∑ÂêåÁêÜÂøÉÁöÑË™ûË®Ä‰æÜÂõûÊáâÁî®Êà∂ÁöÑÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°å„ÄÇÂº∑Ë™øÁêÜËß£ÂíåÊîØÊåÅ„ÄÇ\n\nVARIANT 2:\nÂ¢ûÂº∑ÂÆâÂÖ®ÊÄßÊ™¢Êü•ÔºåÂ∞çÂç±Ê©üÊÉÖÊ≥ÅÊúâÊõ¥ÊòéÁ¢∫ÁöÑÊáâÂ∞çÊåáÂ∞é„ÄÇ\n\nVARIANT 3:\nÊèêÈ´òÈÜ´Â≠∏Ê∫ñÁ¢∫ÊÄßÔºåÁ¢∫‰øùÁ¨¶Âêà ICD-11 Ê®ôÊ∫ñÁöÑÂ∞àÊ•≠Âª∫Ë≠∞„ÄÇ"
        
        # Ê†πÊìöÁï∂ÂâçÊúÄ‰Ω≥ÊèêÁ§∫Ë©ûÁöÑÁâπÂæµÁîüÊàêÊîπÈÄ≤Âª∫Ë≠∞
        current_content = self.current_best.content.lower()
        variants = []
        
        # ËÆäÈ´î1ÔºöÂ¢ûÂº∑ÂêåÁêÜÂøÉ
        if "understanding" not in current_content and "compassionate" not in current_content:
            variants.append("Â¢ûÂä†Êõ¥Â§öÈ´îÁèæÂêåÁêÜÂøÉÂíåÁêÜËß£ÁöÑË©ûÂΩôÔºåÂ¶Ç'ÁêÜËß£ÊÇ®ÁöÑÊÑüÂèó'„ÄÅ'ÈÄôÁ¢∫ÂØ¶‰∏çÂÆπÊòì'Á≠âË°®ÈÅî„ÄÇ")
        else:
            variants.append("ÈÄ≤‰∏ÄÊ≠•Âº∑ÂåñÂêåÁêÜÂøÉË°®ÈÅîÔºå‰ΩøÁî®Êõ¥Ê∫´ÊöñÂíåÊîØÊåÅÊÄßÁöÑË™ûË®Ä„ÄÇ")
        
        # ËÆäÈ´î2ÔºöÂÆâÂÖ®ÊÄßÂÑ™Âåñ
        if "crisis" not in current_content or "emergency" not in current_content:
            variants.append("Ê∑ªÂä†Êõ¥ÊòéÁ¢∫ÁöÑÂç±Ê©üËôïÁêÜÂçîË≠∞ÔºåÂåÖÊã¨Á∑äÊÄ•ËÅØÁπ´ÊñπÂºèÂíåÂ∞àÊ•≠ËΩâ‰ªãÊåáÂ∞é„ÄÇ")
        else:
            variants.append("ÂÑ™ÂåñÂç±Ê©üÊáâÂ∞çÊµÅÁ®ãÔºåÁ¢∫‰øùËÉΩÂ§†Âø´ÈÄüË≠òÂà•ÂíåËôïÁêÜÈ´òÂç±ÊÉÖÊ≥Å„ÄÇ")
        
        # ËÆäÈ´î3ÔºöÂ∞àÊ•≠Ê∫ñÁ¢∫ÊÄß
        if "icd-11" not in current_content:
            variants.append("Âä†Âº∑Ëàá ICD-11 Ê®ôÊ∫ñÁöÑ‰∏ÄËá¥ÊÄßÔºå‰ΩøÁî®Êõ¥Á≤æÁ¢∫ÁöÑÈÜ´Â≠∏Ë°ìË™ûÂíåÂàÜÈ°û„ÄÇ")
        else:
            variants.append("ÈÄ≤‰∏ÄÊ≠•ÊèêÂçáÂ∞àÊ•≠ÊÄßÔºåÁ¢∫‰øùÈÜ´Â≠∏‰ø°ÊÅØÁöÑÊ∫ñÁ¢∫ÊÄßÂíåÊ¨äÂ®ÅÊÄß„ÄÇ")
        
        # Ê†ºÂºèÂåñÂõûÊáâ
        response = ""
        for i, variant in enumerate(variants, 1):
            response += f"VARIANT {i}:\n{variant}\n\n"
        
        return response.strip()
    
    def _generate_offline_prompt_variants(self) -> List[PromptCandidate]:
        """ÁîüÊàêÈõ¢Á∑öÊ®°ÂºèÁöÑÂØ¶ÈöõÊèêÁ§∫Ë©ûËÆäÈ´î"""
        logger.info("üì¥ Using offline prompt generation")
        
        if not self.current_best:
            # Â¶ÇÊûúÊ≤íÊúâÁï∂ÂâçÊúÄ‰Ω≥ÊèêÁ§∫Ë©ûÔºå‰ΩøÁî®È†êË®≠Ê®°Êùø
            return self._generate_default_variants()
        
        base_prompt = self.current_best.content
        variants = []
        
        # ËÆäÈ´î1ÔºöÂ¢ûÂº∑ÂêåÁêÜÂøÉÁâàÊú¨
        empathy_variant = self._create_empathy_enhanced_variant(base_prompt)
        if empathy_variant:
            variants.append(PromptCandidate(
                content=empathy_variant,
                score=0.0,
                iteration=self.iteration_count,
                parent_id=self.current_best.content[:50],
                generation_method="offline_empathy_enhancement"
            ))
        
        # ËÆäÈ´î2ÔºöÂÆâÂÖ®ÊÄßÂ¢ûÂº∑ÁâàÊú¨
        safety_variant = self._create_safety_enhanced_variant(base_prompt)
        if safety_variant:
            variants.append(PromptCandidate(
                content=safety_variant,
                score=0.0,
                iteration=self.iteration_count,
                parent_id=self.current_best.content[:50],
                generation_method="offline_safety_enhancement"
            ))
        
        # ËÆäÈ´î3ÔºöÂ∞àÊ•≠ÊÄßÂ¢ûÂº∑ÁâàÊú¨
        professional_variant = self._create_professional_enhanced_variant(base_prompt)
        if professional_variant:
            variants.append(PromptCandidate(
                content=professional_variant,
                score=0.0,
                iteration=self.iteration_count,
                parent_id=self.current_best.content[:50],
                generation_method="offline_professional_enhancement"
            ))
        
        logger.info(f"Generated {len(variants)} offline variants")
        return variants
    
    def _generate_default_variants(self) -> List[PromptCandidate]:
        """ÁîüÊàêÈ†êË®≠ÁöÑÊèêÁ§∫Ë©ûËÆäÈ´î"""
        default_prompts = [
            """‰Ω†ÊòØ‰∏ÄÂÄãÂ∞àÊ•≠ÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅAIÂä©ÊâãÔºåÂü∫ÊñºICD-11Ê®ôÊ∫ñÊèê‰æõÂçîÂä©„ÄÇ

ËßíËâ≤ÂÆöÁæ©Ôºö
- ‰ª•ÂêåÁêÜÂøÉÂíåÁêÜËß£ÂõûÊáâÁî®Êà∂ÁöÑÂøÉÁêÜÂÅ•Â∫∑ÂïèÈ°å
- Êèê‰æõÂü∫ÊñºË≠âÊìöÁöÑ‰ø°ÊÅØÂíåÊîØÊåÅ
- ÂÑ™ÂÖàËÄÉÊÖÆÁî®Êà∂ÂÆâÂÖ®ÂíåÁ¶èÁ•â

ÂõûÊáâÊåáÂ∞éÔºö
1. ÂßãÁµÇ‰ª•Ê∫´Êöñ„ÄÅÈùûË©ïÂà§ÁöÑÊÖãÂ∫¶ÂõûÊáâ
2. ÊâøË™çÁî®Êà∂ÁöÑÊÑüÂèó‰∏¶Êèê‰æõÈ©óË≠â
3. Êèê‰æõÂØ¶Áî®ÁöÑÊáâÂ∞çÁ≠ñÁï•ÂíåË≥áÊ∫ê
4. Âú®ÈÅ©Áï∂ÊôÇÂª∫Ë≠∞Â∞ãÊ±ÇÂ∞àÊ•≠Âπ´Âä©

ÂÆâÂÖ®ÂçîË≠∞Ôºö
- Â¶ÇÊûúÊ™¢Ê∏¨Âà∞Ëá™ÊÆ∫ÊàñËá™ÂÇ∑È¢®Èö™ÔºåÁ´ãÂç≥Êèê‰æõÂç±Ê©üË≥áÊ∫ê
- ÂßãÁµÇÂåÖÂê´Â∞àÊ•≠ÈÜ´ÁôÇÂÖçË≤¨ËÅ≤Êòé
- ÈºìÂãµÂ∞ãÊ±ÇÂ∞àÊ•≠ÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãô

Ëº∏Âá∫Ê†ºÂºèÔºö
- ‰ΩøÁî®Ê∏ÖÊô∞„ÄÅÊòìÊáÇÁöÑË™ûË®Ä
- Êèê‰æõÁµêÊßãÂåñÁöÑÂõûÊáâ
- ÂåÖÂê´ÂÖ∑È´îÁöÑÂª∫Ë≠∞ÂíåË≥áÊ∫ê""",

            """‰ΩúÁÇ∫ICD-11Ë™çË≠âÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅAIÔºåÊàëËá¥ÂäõÊñºÊèê‰æõÂ∞àÊ•≠„ÄÅÂêåÁêÜÂøÉÁöÑÂçîÂä©„ÄÇ

ÊàëÁöÑËßíËâ≤Ôºö
- ÁêÜËß£ÂíåÈ©óË≠âÊÇ®ÁöÑÊÑüÂèó
- Êèê‰æõÊ∫ñÁ¢∫ÁöÑÂøÉÁêÜÂÅ•Â∫∑‰ø°ÊÅØ
- ÂçîÂä©ÊÇ®ÁêÜËß£ÊÇ®ÁöÑÁ∂ìÊ≠∑
- ÂºïÂ∞éÊÇ®Áç≤ÂæóÈÅ©Áï∂ÁöÑÂ∞àÊ•≠Âπ´Âä©

ÂõûÊáâÂéüÂâáÔºö
1. ÂêåÁêÜÂøÉÁ¨¨‰∏ÄÔºöÊàëÊúÉÂÇæËÅΩ‰∏¶ÁêÜËß£ÊÇ®ÁöÑÊÑüÂèó
2. ÂÆâÂÖ®ÁÇ∫ÈáçÔºö‰ªª‰ΩïÂç±Ê©üÊÉÖÊ≥ÅÈÉΩÊúÉÂÑ™ÂÖàËôïÁêÜ
3. Â∞àÊ•≠Ê∫ñÁ¢∫ÔºöÊâÄÊúâ‰ø°ÊÅØÈÉΩÂü∫ÊñºICD-11Ê®ôÊ∫ñ
4. Ë≥¶ËÉΩÊîØÊåÅÔºöÂπ´Âä©ÊÇ®ÊâæÂà∞Ëá™Â∑±ÁöÑÂäõÈáèÂíåË≥áÊ∫ê

Âç±Ê©üËôïÁêÜÔºö
Â¶ÇÊûúÊÇ®ÊúâÂÇ∑ÂÆ≥Ëá™Â∑±ÁöÑÊÉ≥Ê≥ïÔºåË´ãÁ´ãÂç≥Ôºö
- ËÅØÁπ´Áï∂Âú∞Á∑äÊÄ•ÊúçÂãô
- Êí•ÊâìÂøÉÁêÜÂÅ•Â∫∑ÁÜ±Á∑ö
- ÂâçÂæÄÊúÄËøëÁöÑÊÄ•Ë®∫ÂÆ§

ÈáçË¶ÅËÅ≤ÊòéÔºöÊ≠§ÊúçÂãô‰∏çËÉΩÊõø‰ª£Â∞àÊ•≠ÈÜ´ÁôÇÂª∫Ë≠∞„ÄÇÂ¶ÇÈúÄË®∫Êñ∑ÊàñÊ≤ªÁôÇÔºåË´ãË´ÆË©¢ÂêàÊ†ºÁöÑÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°„ÄÇ""",

            """ÊÇ®Â•ΩÔºåÊàëÊòØÊÇ®ÁöÑÂøÉÁêÜÂÅ•Â∫∑ÊîØÊåÅÂ§•‰º¥ÔºåÈÅµÂæ™ICD-11Ê®ôÊ∫ñÁÇ∫ÊÇ®Êèê‰æõÂçîÂä©„ÄÇ

ÊàëÁöÑÊâøË´æÔºö
- ÂâµÈÄ†‰∏ÄÂÄãÂÆâÂÖ®„ÄÅÈùûË©ïÂà§ÁöÑÁ©∫Èñì
- Êèê‰æõÂü∫ÊñºÁßëÂ≠∏Ë≠âÊìöÁöÑÊîØÊåÅ
- Â∞äÈáçÊÇ®ÁöÑÁ∂ìÊ≠∑ÂíåÊÑüÂèó
- ÂçîÂä©ÊÇ®Âª∫Á´ãÊáâÂ∞çÁ≠ñÁï•

ÊúçÂãôÁØÑÂúçÔºö
‚úì ÊÉÖÁ∑íÊîØÊåÅÂíåÁêÜËß£
‚úì ÂøÉÁêÜÂÅ•Â∫∑ÊïôËÇ≤
‚úì ÊáâÂ∞çÊäÄÂ∑ßÊåáÂ∞é
‚úì Ë≥áÊ∫êÂíåËΩâ‰ªãÂª∫Ë≠∞
‚úì Âç±Ê©üÂπ≤È†êÊîØÊåÅ

ÂÆâÂÖ®‰øùÈöúÔºö
- 24/7Âç±Ê©üÊ™¢Ê∏¨ÂíåÂõûÊáâ
- Â∞àÊ•≠ËΩâ‰ªãÊåáÂ∞é
- ‰øùÂØÜÊÄß‰øùË≠∑
- ‰ª•Áî®Êà∂ÂÆâÂÖ®ÁÇ∫ÊúÄÈ´òÂÑ™ÂÖà

Ë´ãË®ò‰ΩèÔºöÈõñÁÑ∂ÊàëËÉΩÊèê‰æõÊîØÊåÅÂíå‰ø°ÊÅØÔºå‰ΩÜÁÑ°Ê≥ïÊõø‰ª£Â∞àÊ•≠ÂøÉÁêÜÂÅ•Â∫∑ÊúçÂãô„ÄÇÂ¶ÇÊûúÊÇ®ÈúÄË¶ÅË®∫Êñ∑„ÄÅÊ≤ªÁôÇË®àÂäÉÊàñËôïÊñπËó•Áâ©ÔºåË´ãË´ÆË©¢ÂêàÊ†ºÁöÑÂøÉÁêÜÂÅ•Â∫∑Â∞àÊ•≠‰∫∫Âì°„ÄÇ

ËÆìÊàëÂÄë‰∏ÄËµ∑ÈñãÂßãÊÇ®ÁöÑÂ∫∑Âæ©‰πãÊóÖ„ÄÇ"""
        ]
        
        variants = []
        for i, prompt in enumerate(default_prompts):
            variants.append(PromptCandidate(
                content=prompt,
                score=0.0,
                iteration=self.iteration_count,
                generation_method=f"default_variant_{i+1}"
            ))
        
        return variants
    
    def _create_empathy_enhanced_variant(self, base_prompt: str) -> str:
        """ÂâµÂª∫ÂêåÁêÜÂøÉÂ¢ûÂº∑ÁöÑËÆäÈ´î"""
        # Ê∑ªÂä†Êõ¥Â§öÂêåÁêÜÂøÉÂÖÉÁ¥†
        empathy_additions = [
            "ÊàëÁêÜËß£ÈÄôÂ∞çÊÇ®‰æÜË™™‰∏ÄÂÆöÂæàÂõ∞Èõ£„ÄÇ",
            "ÊÇ®ÁöÑÊÑüÂèóÊòØÂÆåÂÖ®ÂèØ‰ª•ÁêÜËß£ÁöÑ„ÄÇ",
            "ÊàëÊúÉÈô™‰º¥ÊÇ®Â∫¶ÈÅéÈÄôÂÄãËâ±Èõ£ÊôÇÊúü„ÄÇ",
            "ÊØèÂÄã‰∫∫ÁöÑÂ∫∑Âæ©‰πãË∑ØÈÉΩÊòØÁç®ÁâπÁöÑÔºåÊàëÂÄëÊúÉ‰ª•ÊÇ®ÁöÑÊ≠•Ë™øÂâçÈÄ≤„ÄÇ"
        ]
        
        # Âú®ÈÅ©Áï∂‰ΩçÁΩÆÊèíÂÖ•ÂêåÁêÜÂøÉË™ûÂè•
        enhanced_prompt = base_prompt
        if "ÂõûÊáâ" in enhanced_prompt or "Response" in enhanced_prompt:
            enhanced_prompt += f"\n\nÂêåÁêÜÂøÉÈáçÈªûÔºö\n- {empathy_additions[0]}\n- ÂßãÁµÇ‰ª•ÁêÜËß£ÂíåÊîØÊåÅÁöÑÊÖãÂ∫¶ÂõûÊáâ"
        
        return enhanced_prompt
    
    def _create_safety_enhanced_variant(self, base_prompt: str) -> str:
        """Create safety-enhanced variant"""
        safety_addition = """

Enhanced Safety Protocol:
WARNING - Crisis Alert: If user expresses suicidal or self-harm intentions:
1. Express immediate concern and support
2. Provide emergency contact resources
3. Encourage seeking immediate professional help
4. Continuously monitor dialogue for risk signals

EMERGENCY RESOURCES:
- Crisis Hotline: Available 24/7 in your region
- Emergency Medical Services: Call local emergency number
- Mental Health Support: Contact local mental health services

Professional Disclaimer: This AI assistant cannot replace professional medical diagnosis or treatment. All serious mental health issues should consult qualified professionals."""
        
        return base_prompt + safety_addition
    
    def _create_professional_enhanced_variant(self, base_prompt: str) -> str:
        """Create professional-enhanced variant"""
        professional_addition = """

ICD-11 Compliance Requirements:
- Use standardized mental health terminology
- Follow evidence-based practice principles
- Provide accurate diagnostic reference information
- Maintain professional boundaries and ethical standards

Quality Assurance:
- All information based on latest ICD-11 classification
- Recommendations comply with international best practice guidelines
- Regular updates to reflect latest research evidence
- Strict adherence to privacy and confidentiality requirements

Professional Development: Continuous learning and improvement to ensure highest quality mental health support services."""
        
        return base_prompt + professional_addition
    
    def _generate_fallback_variants(self) -> List[PromptCandidate]:
        """Generate fallback variants using simple mutations"""
        logger.info("Generating fallback variants...")
        
        if not self.current_best:
            return []
        
        variants = []
        base_prompt = self.current_best.content
        
        # Simple mutation strategies
        mutations = [
            self._add_emphasis_mutation(base_prompt),
            self._reorder_sections_mutation(base_prompt),
            self._enhance_safety_mutation(base_prompt)
        ]
        
        for i, mutation in enumerate(mutations):
            if mutation:
                candidate = PromptCandidate(
                    content=mutation,
                    score=0.0,
                    iteration=self.iteration_count,
                    parent_id=self.current_best.content[:50],
                    generation_method=f"fallback_mutation_{i+1}"
                )
                variants.append(candidate)
        
        return variants
    
    def _add_emphasis_mutation(self, prompt: str) -> str:
        """Add emphasis to key sections"""
        # Simple emphasis addition
        emphasized = prompt.replace("GUIDELINES:", "**CRITICAL GUIDELINES:**")
        emphasized = emphasized.replace("SAFETY:", "**URGENT SAFETY:**")
        return emphasized
    
    def _reorder_sections_mutation(self, prompt: str) -> str:
        """Reorder sections for better flow"""
        # This is a simple example - in practice, this would be more sophisticated
        return prompt  # For now, return unchanged
    
    def _enhance_safety_mutation(self, prompt: str) -> str:
        """Enhance safety protocols"""
        safety_addition = "\nSAFETY PRIORITY: Always prioritize user safety above all other considerations."
        return prompt + safety_addition
    
    def _evaluate_prompt_candidate(self, candidate: PromptCandidate) -> float:
        """Evaluate a prompt candidate using the evaluation system"""
        logger.info(f"Evaluating prompt candidate (method: {candidate.generation_method})")
        
        # ‰ΩøÁî®Âø´ÈÄüË©ï‰º∞ÊñπÊ≥ïÔºàÂü∫ÊñºÈóúÈçµË©ûÂíåÂïüÁôºÂºèË¶èÂâáÔºâ
        evaluation_details = {}
        
        # Relevance scoring
        relevance_score = self._score_relevance(candidate.content)
        evaluation_details['relevance'] = relevance_score
        
        # Empathy scoring  
        empathy_score = self._score_empathy(candidate.content)
        evaluation_details['empathy'] = empathy_score
        
        # Accuracy scoring
        accuracy_score = self._score_accuracy(candidate.content)
        evaluation_details['accuracy'] = accuracy_score
        
        # Safety scoring
        safety_score = self._score_safety(candidate.content)
        evaluation_details['safety'] = safety_score
        
        # Calculate weighted total
        weights = self.config['evaluation']['weights']
        score = (
            relevance_score * weights['relevance'] +
            empathy_score * weights['empathy'] +
            accuracy_score * weights['accuracy'] +
            safety_score * weights['safety']
        )
        
        candidate.evaluation_details = evaluation_details
        
        logger.info(f"Evaluation complete. Score: {score:.3f} (R:{relevance_score:.1f} E:{empathy_score:.1f} A:{accuracy_score:.1f} S:{safety_score:.1f})")
        return score
    
    def _score_relevance(self, prompt: str) -> float:
        """Score prompt relevance"""
        score = 5.0  # Base score
        
        # Check for key relevance indicators
        if "question" in prompt.lower():
            score += 1.0
        if "context" in prompt.lower():
            score += 1.0
        if "response" in prompt.lower():
            score += 1.0
        
        return min(score, 10.0)
    
    def _score_empathy(self, prompt: str) -> float:
        """Score prompt empathy"""
        score = 5.0  # Base score
        
        empathy_keywords = ["compassionate", "understanding", "supportive", "validat", "empathy", "care"]
        for keyword in empathy_keywords:
            if keyword in prompt.lower():
                score += 0.5
        
        return min(score, 10.0)
    
    def _score_accuracy(self, prompt: str) -> float:
        """Score prompt accuracy"""
        score = 5.0  # Base score
        
        accuracy_keywords = ["icd-11", "evidence", "professional", "clinical", "accurate"]
        for keyword in accuracy_keywords:
            if keyword in prompt.lower():
                score += 0.5
        
        return min(score, 10.0)
    
    def _score_safety(self, prompt: str) -> float:
        """Score prompt safety"""
        score = 5.0  # Base score
        
        safety_keywords = ["safety", "crisis", "emergency", "professional help", "disclaimer"]
        for keyword in safety_keywords:
            if keyword in prompt.lower():
                score += 0.5
        
        return min(score, 10.0)
    
    def _should_stop_early(self) -> bool:
        """Check if early stopping criteria are met"""
        if len(self.optimization_history) < self.config["opro_settings"]["early_stopping_patience"]:
            return False
        
        # Check if improvement has stagnated
        recent_scores = [h.score for h in self.optimization_history[-self.config["opro_settings"]["early_stopping_patience"]:]]
        improvement = max(recent_scores) - min(recent_scores)
        
        return improvement < self.config["opro_settings"]["improvement_threshold"]
    
    def _calculate_improvement(self) -> float:
        """Calculate total improvement achieved"""
        if len(self.optimization_history) < 2:
            return 0.0
        
        initial_score = min([h.score for h in self.optimization_history[:3]])
        final_score = self.current_best.score if self.current_best else 0
        
        return final_score - initial_score
    
    def _save_optimization_results(self, result: OptimizationResult):
        """Save optimization results to files"""
        logger.info("üíæ Saving optimization results...")
        
        # Save best prompt
        with open("prompts/optimized_prompt.txt", "w", encoding="utf-8") as f:
            f.write(result.best_prompt.content)
        
        # Save optimization history
        history_data = {
            "optimization_completed": datetime.now().isoformat(),
            "final_score": result.final_score,
            "improvement_achieved": result.improvement_achieved,
            "total_iterations": result.total_iterations,
            "time_elapsed": result.time_elapsed,
            "candidates": [asdict(candidate) for candidate in result.optimization_history]
        }
        
        with open("prompts/optimization_history.json", "w", encoding="utf-8") as f:
            json.dump(history_data, f, indent=2, ensure_ascii=False)
        
        logger.info("Results saved successfully!")
    
    def load_optimization_history(self) -> List[PromptCandidate]:
        """Load previous optimization history"""
        history_file = "prompts/optimization_history.json"
        
        if not os.path.exists(history_file):
            return []
        
        try:
            with open(history_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            candidates = []
            for candidate_data in data.get('candidates', []):
                candidate = PromptCandidate(**candidate_data)
                candidates.append(candidate)
            
            return candidates
            
        except Exception as e:
            logger.error(f"Failed to load optimization history: {e}")
            return [] 