# OPRO LLM评分系统指南

## 🧠 LLM智能评分

系统现在使用Llama模型来评分提示词，而不是简单的数字规则。

## 📊 评分标准

LLM会根据以下5个维度评分（0-10分）：

| 维度 | 权重 | 说明 |
|------|------|------|
| **EMPATHY** | 25% | 是否鼓励同理心和同情心的回应 |
| **PROFESSIONALISM** | 20% | 是否维持专业医疗标准 |
| **CLARITY** | 20% | 指令是否清晰且结构良好 |
| **SAFETY** | 20% | 是否包含适当的安全准则 |
| **EFFECTIVENESS** | 15% | 是否能产生有用、可行的回应 |

## 🎯 评分等级

- **9-10分**: 优秀 - 在所有标准中表现卓越
- **7-8分**: 良好 - 在大多数方面表现强劲
- **5-6分**: 平均 - 有一些优势但需要改进
- **3-4分**: 低于平均 - 有明显弱点
- **1-2分**: 差 - 有重大问题
- **0分**: 完全不合格

## 🔧 技术实现

### LLM评分流程
1. **主要方法**: 使用Llama模型智能评分
2. **备用方法**: 如果LLM失败，自动回退到规则评分
3. **评分记录**: 保存LLM的评分理由和详细反馈

### 示例LLM评分输出
```
SCORE: 8.5
REASON: 这个提示词在同理心和专业性方面表现出色，包含了清晰的指导原则和安全考虑。结构良好且易于理解，能够指导产生高质量的心理健康回应。
```

## 📈 优势对比

| 特征 | 数字规则 | LLM评分 |
|------|----------|---------|
| **理解深度** | 关键词匹配 | 语义理解 |
| **评分准确性** | 基础 | 高级 |
| **评分理由** | 无 | 详细解释 |
| **适应性** | 固定 | 智能调整 |
| **专业性** | 有限 | 专家级别 |

## 🚀 使用方法

### 运行LLM评分的OPRO
```bash
# 快速测试（2次迭代）
python run_opro.py --mode test-run

# 完整优化（8次迭代）
python run_opro.py --mode optimize
```

### 查看详细评分信息
```bash
# 查看优化历史（包含LLM评分理由）
cat prompts/optimization_history.json
```

## 📊 输出格式

优化历史中每个候选项现在包含：

```json
{
  "content": "提示词内容...",
  "score": 8.5,
  "evaluation_details": {
    "evaluation_method": "llm",
    "score": 8.5,
    "reason": "LLM的详细评分理由...",
    "llm_response": "完整的LLM回应..."
  }
}
```

## ⚡ 性能说明

- **初次评分**: 需要加载模型（约15-20秒）
- **后续评分**: 每次约2-3秒
- **自动缓存**: 模型只加载一次
- **自动备用**: LLM失败时使用规则评分

## 🎯 预期改进

使用LLM评分后，你应该看到：
- 更准确的提示词质量评估
- 更有意义的分数差异
- 详细的改进建议
- 更好的优化收敛

## 🔍 调试信息

运行时会显示：
```
DEBUG: Using LLM to evaluate prompt...
DEBUG: LLM Score: 8.5
DEBUG: LLM Reason: 这个提示词在同理心方面表现出色...
```

这样你就能实时看到LLM的评分过程和理由！ 