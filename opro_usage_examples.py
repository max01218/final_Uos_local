#!/usr/bin/env python3
"""
OPROç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹
æ¼”ç¤ºä¸åŒçš„è°ƒç”¨æ–¹æ³•å’Œå®é™…åº”ç”¨åœºæ™¯
"""

import os
import subprocess
import time
from pathlib import Path

def run_command(cmd, description):
    """è¿è¡Œå‘½ä»¤å¹¶æ˜¾ç¤ºç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ”¥ {description}")
    print(f"ğŸ’» å‘½ä»¤: {cmd}")
    print(f"{'='*60}")
    
    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)
    
    if result.returncode == 0:
        print(f"âœ… æˆåŠŸæ‰§è¡Œ")
        if result.stdout:
            print(f"ğŸ“¤ è¾“å‡º:\n{result.stdout}")
    else:
        print(f"âŒ æ‰§è¡Œå¤±è´¥")
        if result.stderr:
            print(f"ğŸ“¤ é”™è¯¯:\n{result.stderr}")
    
    return result.returncode == 0

def main():
    """ä¸»è¦ä½¿ç”¨ç¤ºä¾‹"""
    
    print("ğŸ§  OPROç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹")
    print("=" * 60)
    
    # ç¤ºä¾‹1: å¿«é€Ÿç³»ç»Ÿæ£€æŸ¥
    print("\nğŸ“‹ ç¤ºä¾‹1: ç³»ç»ŸçŠ¶æ€æ£€æŸ¥")
    run_command(
        "cd OPRO_Streamlined && python run_opro.py --mode info",
        "æ£€æŸ¥OPROç³»ç»ŸçŠ¶æ€å’ŒGPUé…ç½®"
    )
    
    # ç¤ºä¾‹2: GPUçŠ¶æ€æ£€æŸ¥
    print("\nğŸ“‹ ç¤ºä¾‹2: GPUæ€§èƒ½æ£€æŸ¥") 
    run_command(
        "cd OPRO_Streamlined && python run_opro.py --gpu-check",
        "æ£€æŸ¥GPUå¯ç”¨æ€§å’Œæ¨èæ¨¡å‹é…ç½®"
    )
    
    # ç¤ºä¾‹3: å¿«é€Ÿæµ‹è¯•ä¼˜åŒ–
    print("\nğŸ“‹ ç¤ºä¾‹3: å¿«é€Ÿæµ‹è¯•ä¼˜åŒ– (æ¨è)")
    run_command(
        "cd OPRO_Streamlined && python run_opro.py --mode test-run",
        "è¿è¡Œ2è½®å¿«é€Ÿæµ‹è¯•ï¼ŒéªŒè¯ç³»ç»Ÿå·¥ä½œçŠ¶æ€"
    )
    
    # ç¤ºä¾‹4: è¯„ä¼°å½“å‰æç¤ºè¯
    print("\nğŸ“‹ ç¤ºä¾‹4: è¯„ä¼°å½“å‰æç¤ºè¯")
    run_command(
        "cd OPRO_Streamlined && python run_opro.py --mode evaluate --prompt-file prompts/optimized_prompt.txt",
        "è¯„ä¼°å½“å‰ä½¿ç”¨çš„ä¼˜åŒ–æç¤ºè¯æ€§èƒ½"
    )
    
    # ç¤ºä¾‹5: æµ‹è¯•LLMæ¨¡å‹
    print("\nğŸ“‹ ç¤ºä¾‹5: æµ‹è¯•LLMæ¨¡å‹è¿æ¥")
    run_command(
        "cd OPRO_Streamlined && python run_opro.py --mode test-model",
        "æµ‹è¯•æœ¬åœ°LLMæ¨¡å‹æ˜¯å¦å¯ä»¥æ­£å¸¸åŠ è½½å’Œæ¨ç†"
    )
    
    print("\nğŸ¯ é«˜çº§ç”¨æ³•æç¤º:")
    print("   å®Œæ•´ä¼˜åŒ–: cd OPRO_Streamlined && python run_opro.py --mode optimize")
    print("   è‡ªåŠ¨è°ƒåº¦: python auto_opro_scheduler.py")
    print("   åŸå§‹ç‰ˆæœ¬: cd ICD11_OPRO && python run_opro.py")
    
    print("\nğŸ“Š ä¼˜åŒ–ç»“æœæŸ¥çœ‹:")
    print("   å½“å‰æç¤ºè¯: OPRO_Streamlined/prompts/optimized_prompt.txt")  
    print("   ä¼˜åŒ–å†å²: OPRO_Streamlined/prompts/optimization_history.json")
    print("   ç”¨æˆ·åé¦ˆ: interactions.json")

def show_optimization_workflow():
    """å±•ç¤ºå®Œæ•´çš„ä¼˜åŒ–å·¥ä½œæµç¨‹"""
    
    workflow_steps = [
        "1. ğŸ” ç³»ç»Ÿæ£€æŸ¥: python run_opro.py --mode info",
        "2. ğŸš€ GPUæ£€æŸ¥: python run_opro.py --gpu-check", 
        "3. ğŸ§ª å¿«é€Ÿæµ‹è¯•: python run_opro.py --mode test-run",
        "4. ğŸ“Š æ€§èƒ½è¯„ä¼°: python run_opro.py --mode evaluate --prompt-file prompts/optimized_prompt.txt",
        "5. ğŸ† å®Œæ•´ä¼˜åŒ–: python run_opro.py --mode optimize",
        "6. ğŸ”„ è‡ªåŠ¨è°ƒåº¦: python auto_opro_scheduler.py (åå°è¿è¡Œ)"
    ]
    
    print("\nğŸ”¥ OPROä¼˜åŒ–å®Œæ•´å·¥ä½œæµç¨‹:")
    print("=" * 50)
    
    for step in workflow_steps:
        print(f"   {step}")
    
    print("\nğŸ’¡ å»ºè®®:")
    print("   â€¢ é¦–æ¬¡ä½¿ç”¨å…ˆè¿è¡Œæ­¥éª¤1-3éªŒè¯ç³»ç»Ÿ")
    print("   â€¢ å®šæœŸè¿è¡Œæ­¥éª¤4æ£€æŸ¥æç¤ºè¯æ€§èƒ½")
    print("   â€¢ æ”¶é›†è¶³å¤Ÿåé¦ˆåè¿è¡Œæ­¥éª¤5ä¼˜åŒ–")
    print("   â€¢ ç”Ÿäº§ç¯å¢ƒå¯ç”¨æ­¥éª¤6è‡ªåŠ¨è°ƒåº¦")

def check_opro_status():
    """æ£€æŸ¥OPROå½“å‰çŠ¶æ€"""
    
    print("\nğŸ” OPROç³»ç»ŸçŠ¶æ€æ£€æŸ¥:")
    print("=" * 40)
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    key_files = {
        "OPRO_Streamlined/prompts/optimized_prompt.txt": "å½“å‰ä¼˜åŒ–æç¤ºè¯",
        "OPRO_Streamlined/prompts/optimization_history.json": "ä¼˜åŒ–å†å²è®°å½•",
        "OPRO_Streamlined/config/config.json": "OPROé…ç½®æ–‡ä»¶",
        "interactions.json": "ç”¨æˆ·äº¤äº’åé¦ˆ",
        "opro_scheduler_state.json": "è°ƒåº¦å™¨çŠ¶æ€",
        "opro_scheduler.log": "è°ƒåº¦å™¨æ—¥å¿—"
    }
    
    for file_path, description in key_files.items():
        if os.path.exists(file_path):
            file_size = os.path.getsize(file_path)
            mod_time = os.path.getmtime(file_path)
            mod_date = time.strftime("%Y-%m-%d %H:%M", time.localtime(mod_time))
            print(f"âœ… {description}")
            print(f"   æ–‡ä»¶: {file_path}")
            print(f"   å¤§å°: {file_size} bytes, ä¿®æ”¹: {mod_date}")
        else:
            print(f"âŒ {description}")
            print(f"   æ–‡ä»¶: {file_path} (ä¸å­˜åœ¨)")
    
    # æ£€æŸ¥å½“å‰æç¤ºè¯å†…å®¹
    prompt_file = "OPRO_Streamlined/prompts/optimized_prompt.txt"
    if os.path.exists(prompt_file):
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()
        print(f"\nğŸ“„ å½“å‰ä¼˜åŒ–æç¤ºè¯é¢„è§ˆ:")
        print(f"   é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"   å†…å®¹: {content[:200]}...")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1:
        if sys.argv[1] == "examples":
            main()
        elif sys.argv[1] == "workflow":
            show_optimization_workflow()
        elif sys.argv[1] == "status":
            check_opro_status()
        else:
            print("ç”¨æ³•: python opro_usage_examples.py [examples|workflow|status]")
    else:
        print("ğŸ§  OPROä½¿ç”¨æŒ‡å—")
        print("=" * 30)
        print("python opro_usage_examples.py examples  - è¿è¡Œä½¿ç”¨ç¤ºä¾‹")
        print("python opro_usage_examples.py workflow  - æ˜¾ç¤ºä¼˜åŒ–å·¥ä½œæµç¨‹") 
        print("python opro_usage_examples.py status    - æ£€æŸ¥ç³»ç»ŸçŠ¶æ€")
        
        # é»˜è®¤æ˜¾ç¤ºå·¥ä½œæµç¨‹
        show_optimization_workflow()
        check_opro_status() 